{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screeenplay Genre Classifier\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "In this section, I split the data into training and test sets. Then I do text preprocessing on both sets using WordNetLemmation from my custom class TextPreprocesser located in the preprocessing.py. After the lemmatization of both training and test sets, I find the intersection of the 300 most common words by genre in the training set to create a custom stop word list. That stop word list is then applied to the testing set to avoid any data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import TextPreprocessor #custom classes\n",
    "from topicmodels import Modeling #custom classes\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Step: splitting data into train and test sets to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/cleaned_data.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crime',\n",
       " 'Romance',\n",
       " 'Animation',\n",
       " 'SciFi',\n",
       " 'Fantasy',\n",
       " 'History',\n",
       " 'Action',\n",
       " 'Drama',\n",
       " 'War',\n",
       " 'Thriller',\n",
       " 'Mystery',\n",
       " 'Documentary',\n",
       " 'Horror',\n",
       " 'Family',\n",
       " 'Adventure',\n",
       " 'Music',\n",
       " 'Comedy',\n",
       " 'Western']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generating cols for target sets\n",
    "genre_list = list(data.columns[-18:])\n",
    "genre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data\n",
    "X = data.loc[:, ['title', 'text']].copy()\n",
    "y = data.loc[:, genre_list].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Step: Lemmatizing and removing basic stop words from target and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextPreprocessor(activator_type='wnl', lem_or_stem='lem') #custom preprocessing class\n",
    "\n",
    "X_train_lem = X_train.text.apply(lambda x: tp.lem_process_doc(x)) #this function lemmatizes and removes stop words from the nltk pacakge\n",
    "X_test_lem = X_test.text.apply(lambda x: tp.lem_process_doc(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Step: Finding the intersection of stop words from the X_train set and removing those words from both X_train and X_test\n",
    "\n",
    "*Finding the intersection of stopwords in the X_train set will avoid data leakage because it assumes no information about the words in the X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train_lem, y_train], axis=1) #concating to find stop words by genre\n",
    "\n",
    "genre_dfs = {} #creating a dict of dataframes by genre\n",
    "\n",
    "for i in genre_list:\n",
    "    genre_dfs[i] = df_train[df_train[i] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dct = {} #creating a dict to store word frequency by genre\n",
    "\n",
    "for i in genre_list:\n",
    "\n",
    "    freq_dct[i] = FreqDist((\" \".join(genre_dfs[i].text)).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get most frequent words in freqdist\n",
    "def getting_stops(freq_dist, num): \n",
    "\n",
    "    \"\"\"\n",
    "    This function takes in a frequency distrubtion, a list, and a number and returns the number of words by count.\n",
    "    This function will help in created an optimial stop_word list.\n",
    "\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    \n",
    "    for i in freq_dist.most_common(num):\n",
    "        lst.append(i[0])\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [] #appending sets of common words by genre to list\n",
    "\n",
    "for i in genre_list:\n",
    "    \n",
    "    lst.append(set(getting_stops(freq_dct[i], 500))) #500 words chosen based on eda in previous notebook\n",
    "\n",
    "stops = list(set.intersection(*lst)) #finding the intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/names.txt') as f: #this file contains common names of people\n",
    "    line = f.readlines()\n",
    "\n",
    "for i in line:\n",
    "    stops.append(i.strip('\\n').lower()) #appending names to the stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping(row, stops):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes in text and a list of stop words. \n",
    "    It returns updated text without the stopwords specified in the argument\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    row_split = row.split()\n",
    "    updated_row = [x for x in row_split if x not in stops]\n",
    "\n",
    "    return \" \".join(updated_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_stops = X_train_lem.apply(lambda x: stopping(x, stops)) #applying new stop words to training data\n",
    "X_test_no_stops = X_test_lem.apply(lambda x: stopping(x, stops)) #applying new stop words to testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_stops_title = pd.concat([X_train_no_stops, X_train.title], axis=1) #to keep track of title of each screenplay\n",
    "X_test_no_stops_title = pd.concat([X_test_no_stops, X_test.title], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data for model building and testing\n",
    "X_train_no_stops_title.to_csv('data/X_train.csv')\n",
    "X_test_no_stops_title.to_csv('data/X_test.csv')\n",
    "y_train.to_csv('data/y_train.csv')\n",
    "y_test.to_csv('data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (contest)",
   "language": "python",
   "name": "contest_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
